name: "Cifar-Resnet" 
layer { # train data layer 
  name: "dataLayer" 
  type: "Data" 
  top: "data" 
  top: "label" 
  include { 
    phase: TRAIN 
  } 
  transform_param { 
    mirror: true 
    crop_size: 32 
    mean_file: "examples/cifar10/mean.binaryproto" 
  } 
  data_param { 
    source: "examples/cifar10/cifar10_train_lmdb" 
#    source: "examples/cifar10/cifar10_train10k_lmdb" 
    batch_size: 125
    backend: LMDB 
  } 
  image_data_param { 
  shuffle: true 
  } 
} 
layer { # test data layer 
  name: "dataLayer" 
  type: "Data" 
  top: "data" 
  top: "label" 
  include { 
    phase: TEST 
  } 
  transform_param { 
    mirror: false 
    crop_size: 32 
    mean_file: "examples/cifar10/mean.binaryproto" 
  } 
  data_param { 
    source: "examples/cifar10/cifar10_test_lmdb" 
    batch_size: 125 
    backend: LMDB 
  } 
} 
layer { # conv 
  name: "conv" 
  type: "Convolution" 
  bottom: "data" 
  top: "conv" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_conv 
  name: "batchNorm_conv" 
  type: "BatchNorm" 
  bottom: "conv" 
  top: "bn_conv" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_conv 
  name: "batchNorm_conv" 
  type: "BatchNorm" 
  bottom: "conv" 
  top: "bn_conv" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_conv 
  name: "scale_conv" 
  type: "Scale" 
  bottom: "bn_conv" 
  top: "bn_conv" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_conv 
  name: "relu_bn_conv" 
  type: "ReLU" 
  bottom: "bn_conv" 
  top: "bn_conv" 
} 
layer { # Conv16_1 
  name: "Conv16_1" 
  type: "Convolution" 
  bottom: "bn_conv" 
  top: "Conv16_1" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_1 
  name: "batchNorm_Conv16_1" 
  type: "BatchNorm" 
  bottom: "Conv16_1" 
  top: "bn_Conv16_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_1 
  name: "batchNorm_Conv16_1" 
  type: "BatchNorm" 
  bottom: "Conv16_1" 
  top: "bn_Conv16_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_1 
  name: "scale_Conv16_1" 
  type: "Scale" 
  bottom: "bn_Conv16_1" 
  top: "bn_Conv16_1" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_1 
  name: "relu_bn_Conv16_1" 
  type: "ReLU" 
  bottom: "bn_Conv16_1" 
  top: "bn_Conv16_1" 
} 
layer { # Conv16_1_b 
  name: "Conv16_1_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_1" 
  top: "Conv16_1_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_1_b 
  name: "batchNorm_Conv16_1_b" 
  type: "BatchNorm" 
  bottom: "Conv16_1_b" 
  top: "bn_Conv16_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_1_b 
  name: "batchNorm_Conv16_1_b" 
  type: "BatchNorm" 
  bottom: "Conv16_1_b" 
  top: "bn_Conv16_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_1_b 
  name: "scale_Conv16_1_b" 
  type: "Scale" 
  bottom: "bn_Conv16_1_b" 
  top: "bn_Conv16_1_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_bn_conv 
    name: "sum_bn_conv" 
    type: "Eltwise" 
    bottom: "bn_conv" 
    bottom: "bn_Conv16_1_b" 
    top: "sum_bn_Conv16_1_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_1_b 
  name: "relu_sum_bn_Conv16_1_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_1_b" 
  top: "sum_bn_Conv16_1_b" 
} 
layer { # Conv16_2 
  name: "Conv16_2" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_1_b" 
  top: "Conv16_2" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_2 
  name: "batchNorm_Conv16_2" 
  type: "BatchNorm" 
  bottom: "Conv16_2" 
  top: "bn_Conv16_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_2 
  name: "batchNorm_Conv16_2" 
  type: "BatchNorm" 
  bottom: "Conv16_2" 
  top: "bn_Conv16_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_2 
  name: "scale_Conv16_2" 
  type: "Scale" 
  bottom: "bn_Conv16_2" 
  top: "bn_Conv16_2" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_2 
  name: "relu_bn_Conv16_2" 
  type: "ReLU" 
  bottom: "bn_Conv16_2" 
  top: "bn_Conv16_2" 
} 
layer { # Conv16_2_b 
  name: "Conv16_2_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_2" 
  top: "Conv16_2_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_2_b 
  name: "batchNorm_Conv16_2_b" 
  type: "BatchNorm" 
  bottom: "Conv16_2_b" 
  top: "bn_Conv16_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_2_b 
  name: "batchNorm_Conv16_2_b" 
  type: "BatchNorm" 
  bottom: "Conv16_2_b" 
  top: "bn_Conv16_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_2_b 
  name: "scale_Conv16_2_b" 
  type: "Scale" 
  bottom: "bn_Conv16_2_b" 
  top: "bn_Conv16_2_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_1_b 
    name: "sum_sum_bn_Conv16_1_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_1_b" 
    bottom: "bn_Conv16_2_b" 
    top: "sum_bn_Conv16_2_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_2_b 
  name: "relu_sum_bn_Conv16_2_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_2_b" 
  top: "sum_bn_Conv16_2_b" 
} 
layer { # Conv16_3 
  name: "Conv16_3" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_2_b" 
  top: "Conv16_3" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_3 
  name: "batchNorm_Conv16_3" 
  type: "BatchNorm" 
  bottom: "Conv16_3" 
  top: "bn_Conv16_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_3 
  name: "batchNorm_Conv16_3" 
  type: "BatchNorm" 
  bottom: "Conv16_3" 
  top: "bn_Conv16_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_3 
  name: "scale_Conv16_3" 
  type: "Scale" 
  bottom: "bn_Conv16_3" 
  top: "bn_Conv16_3" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_3 
  name: "relu_bn_Conv16_3" 
  type: "ReLU" 
  bottom: "bn_Conv16_3" 
  top: "bn_Conv16_3" 
} 
layer { # Conv16_3_b 
  name: "Conv16_3_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_3" 
  top: "Conv16_3_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_3_b 
  name: "batchNorm_Conv16_3_b" 
  type: "BatchNorm" 
  bottom: "Conv16_3_b" 
  top: "bn_Conv16_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_3_b 
  name: "batchNorm_Conv16_3_b" 
  type: "BatchNorm" 
  bottom: "Conv16_3_b" 
  top: "bn_Conv16_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_3_b 
  name: "scale_Conv16_3_b" 
  type: "Scale" 
  bottom: "bn_Conv16_3_b" 
  top: "bn_Conv16_3_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_2_b 
    name: "sum_sum_bn_Conv16_2_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_2_b" 
    bottom: "bn_Conv16_3_b" 
    top: "sum_bn_Conv16_3_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_3_b 
  name: "relu_sum_bn_Conv16_3_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_3_b" 
  top: "sum_bn_Conv16_3_b" 
} 
layer { # Conv16_4 
  name: "Conv16_4" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_3_b" 
  top: "Conv16_4" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_4 
  name: "batchNorm_Conv16_4" 
  type: "BatchNorm" 
  bottom: "Conv16_4" 
  top: "bn_Conv16_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_4 
  name: "batchNorm_Conv16_4" 
  type: "BatchNorm" 
  bottom: "Conv16_4" 
  top: "bn_Conv16_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_4 
  name: "scale_Conv16_4" 
  type: "Scale" 
  bottom: "bn_Conv16_4" 
  top: "bn_Conv16_4" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_4 
  name: "relu_bn_Conv16_4" 
  type: "ReLU" 
  bottom: "bn_Conv16_4" 
  top: "bn_Conv16_4" 
} 
layer { # Conv16_4_b 
  name: "Conv16_4_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_4" 
  top: "Conv16_4_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_4_b 
  name: "batchNorm_Conv16_4_b" 
  type: "BatchNorm" 
  bottom: "Conv16_4_b" 
  top: "bn_Conv16_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_4_b 
  name: "batchNorm_Conv16_4_b" 
  type: "BatchNorm" 
  bottom: "Conv16_4_b" 
  top: "bn_Conv16_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_4_b 
  name: "scale_Conv16_4_b" 
  type: "Scale" 
  bottom: "bn_Conv16_4_b" 
  top: "bn_Conv16_4_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_3_b 
    name: "sum_sum_bn_Conv16_3_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_3_b" 
    bottom: "bn_Conv16_4_b" 
    top: "sum_bn_Conv16_4_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_4_b 
  name: "relu_sum_bn_Conv16_4_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_4_b" 
  top: "sum_bn_Conv16_4_b" 
} 
layer { # Conv16_5 
  name: "Conv16_5" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_4_b" 
  top: "Conv16_5" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_5 
  name: "batchNorm_Conv16_5" 
  type: "BatchNorm" 
  bottom: "Conv16_5" 
  top: "bn_Conv16_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_5 
  name: "batchNorm_Conv16_5" 
  type: "BatchNorm" 
  bottom: "Conv16_5" 
  top: "bn_Conv16_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_5 
  name: "scale_Conv16_5" 
  type: "Scale" 
  bottom: "bn_Conv16_5" 
  top: "bn_Conv16_5" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_5 
  name: "relu_bn_Conv16_5" 
  type: "ReLU" 
  bottom: "bn_Conv16_5" 
  top: "bn_Conv16_5" 
} 
layer { # Conv16_5_b 
  name: "Conv16_5_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_5" 
  top: "Conv16_5_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_5_b 
  name: "batchNorm_Conv16_5_b" 
  type: "BatchNorm" 
  bottom: "Conv16_5_b" 
  top: "bn_Conv16_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_5_b 
  name: "batchNorm_Conv16_5_b" 
  type: "BatchNorm" 
  bottom: "Conv16_5_b" 
  top: "bn_Conv16_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_5_b 
  name: "scale_Conv16_5_b" 
  type: "Scale" 
  bottom: "bn_Conv16_5_b" 
  top: "bn_Conv16_5_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_4_b 
    name: "sum_sum_bn_Conv16_4_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_4_b" 
    bottom: "bn_Conv16_5_b" 
    top: "sum_bn_Conv16_5_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_5_b 
  name: "relu_sum_bn_Conv16_5_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_5_b" 
  top: "sum_bn_Conv16_5_b" 
} 
layer { # Conv16_6 
  name: "Conv16_6" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_5_b" 
  top: "Conv16_6" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_6 
  name: "batchNorm_Conv16_6" 
  type: "BatchNorm" 
  bottom: "Conv16_6" 
  top: "bn_Conv16_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_6 
  name: "batchNorm_Conv16_6" 
  type: "BatchNorm" 
  bottom: "Conv16_6" 
  top: "bn_Conv16_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_6 
  name: "scale_Conv16_6" 
  type: "Scale" 
  bottom: "bn_Conv16_6" 
  top: "bn_Conv16_6" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_6 
  name: "relu_bn_Conv16_6" 
  type: "ReLU" 
  bottom: "bn_Conv16_6" 
  top: "bn_Conv16_6" 
} 
layer { # Conv16_6_b 
  name: "Conv16_6_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_6" 
  top: "Conv16_6_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_6_b 
  name: "batchNorm_Conv16_6_b" 
  type: "BatchNorm" 
  bottom: "Conv16_6_b" 
  top: "bn_Conv16_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_6_b 
  name: "batchNorm_Conv16_6_b" 
  type: "BatchNorm" 
  bottom: "Conv16_6_b" 
  top: "bn_Conv16_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_6_b 
  name: "scale_Conv16_6_b" 
  type: "Scale" 
  bottom: "bn_Conv16_6_b" 
  top: "bn_Conv16_6_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_5_b 
    name: "sum_sum_bn_Conv16_5_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_5_b" 
    bottom: "bn_Conv16_6_b" 
    top: "sum_bn_Conv16_6_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_6_b 
  name: "relu_sum_bn_Conv16_6_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_6_b" 
  top: "sum_bn_Conv16_6_b" 
} 
layer { # Conv16_7 
  name: "Conv16_7" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_6_b" 
  top: "Conv16_7" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_7 
  name: "batchNorm_Conv16_7" 
  type: "BatchNorm" 
  bottom: "Conv16_7" 
  top: "bn_Conv16_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_7 
  name: "batchNorm_Conv16_7" 
  type: "BatchNorm" 
  bottom: "Conv16_7" 
  top: "bn_Conv16_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_7 
  name: "scale_Conv16_7" 
  type: "Scale" 
  bottom: "bn_Conv16_7" 
  top: "bn_Conv16_7" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_7 
  name: "relu_bn_Conv16_7" 
  type: "ReLU" 
  bottom: "bn_Conv16_7" 
  top: "bn_Conv16_7" 
} 
layer { # Conv16_7_b 
  name: "Conv16_7_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_7" 
  top: "Conv16_7_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_7_b 
  name: "batchNorm_Conv16_7_b" 
  type: "BatchNorm" 
  bottom: "Conv16_7_b" 
  top: "bn_Conv16_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_7_b 
  name: "batchNorm_Conv16_7_b" 
  type: "BatchNorm" 
  bottom: "Conv16_7_b" 
  top: "bn_Conv16_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_7_b 
  name: "scale_Conv16_7_b" 
  type: "Scale" 
  bottom: "bn_Conv16_7_b" 
  top: "bn_Conv16_7_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_6_b 
    name: "sum_sum_bn_Conv16_6_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_6_b" 
    bottom: "bn_Conv16_7_b" 
    top: "sum_bn_Conv16_7_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_7_b 
  name: "relu_sum_bn_Conv16_7_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_7_b" 
  top: "sum_bn_Conv16_7_b" 
} 
layer { # Conv16_8 
  name: "Conv16_8" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_7_b" 
  top: "Conv16_8" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_8 
  name: "batchNorm_Conv16_8" 
  type: "BatchNorm" 
  bottom: "Conv16_8" 
  top: "bn_Conv16_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_8 
  name: "batchNorm_Conv16_8" 
  type: "BatchNorm" 
  bottom: "Conv16_8" 
  top: "bn_Conv16_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_8 
  name: "scale_Conv16_8" 
  type: "Scale" 
  bottom: "bn_Conv16_8" 
  top: "bn_Conv16_8" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_8 
  name: "relu_bn_Conv16_8" 
  type: "ReLU" 
  bottom: "bn_Conv16_8" 
  top: "bn_Conv16_8" 
} 
layer { # Conv16_8_b 
  name: "Conv16_8_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_8" 
  top: "Conv16_8_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_8_b 
  name: "batchNorm_Conv16_8_b" 
  type: "BatchNorm" 
  bottom: "Conv16_8_b" 
  top: "bn_Conv16_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_8_b 
  name: "batchNorm_Conv16_8_b" 
  type: "BatchNorm" 
  bottom: "Conv16_8_b" 
  top: "bn_Conv16_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_8_b 
  name: "scale_Conv16_8_b" 
  type: "Scale" 
  bottom: "bn_Conv16_8_b" 
  top: "bn_Conv16_8_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_7_b 
    name: "sum_sum_bn_Conv16_7_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_7_b" 
    bottom: "bn_Conv16_8_b" 
    top: "sum_bn_Conv16_8_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_8_b 
  name: "relu_sum_bn_Conv16_8_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_8_b" 
  top: "sum_bn_Conv16_8_b" 
} 
layer { # Conv16_9 
  name: "Conv16_9" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_8_b" 
  top: "Conv16_9" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_9 
  name: "batchNorm_Conv16_9" 
  type: "BatchNorm" 
  bottom: "Conv16_9" 
  top: "bn_Conv16_9" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_9 
  name: "batchNorm_Conv16_9" 
  type: "BatchNorm" 
  bottom: "Conv16_9" 
  top: "bn_Conv16_9" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_9 
  name: "scale_Conv16_9" 
  type: "Scale" 
  bottom: "bn_Conv16_9" 
  top: "bn_Conv16_9" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_Conv16_9 
  name: "relu_bn_Conv16_9" 
  type: "ReLU" 
  bottom: "bn_Conv16_9" 
  top: "bn_Conv16_9" 
} 
layer { # Conv16_9_b 
  name: "Conv16_9_b" 
  type: "Convolution" 
  bottom: "bn_Conv16_9" 
  top: "Conv16_9_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_Conv16_9_b 
  name: "batchNorm_Conv16_9_b" 
  type: "BatchNorm" 
  bottom: "Conv16_9_b" 
  top: "bn_Conv16_9_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_Conv16_9_b 
  name: "batchNorm_Conv16_9_b" 
  type: "BatchNorm" 
  bottom: "Conv16_9_b" 
  top: "bn_Conv16_9_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_Conv16_9_b 
  name: "scale_Conv16_9_b" 
  type: "Scale" 
  bottom: "bn_Conv16_9_b" 
  top: "bn_Conv16_9_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_Conv16_8_b 
    name: "sum_sum_bn_Conv16_8_b" 
    type: "Eltwise" 
    bottom: "sum_bn_Conv16_8_b" 
    bottom: "bn_Conv16_9_b" 
    top: "sum_bn_Conv16_9_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_Conv16_9_b 
  name: "relu_sum_bn_Conv16_9_b" 
  type: "ReLU" 
  bottom: "sum_bn_Conv16_9_b" 
  top: "sum_bn_Conv16_9_b" 
} 
layer { # resblk32 
  name: "resblk32" 
  type: "Convolution" 
  bottom: "sum_bn_Conv16_9_b" 
  top: "resblk32" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 2 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32 
  name: "batchNorm_resblk32" 
  type: "BatchNorm" 
  bottom: "resblk32" 
  top: "bn_resblk32" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32 
  name: "batchNorm_resblk32" 
  type: "BatchNorm" 
  bottom: "resblk32" 
  top: "bn_resblk32" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32 
  name: "scale_resblk32" 
  type: "Scale" 
  bottom: "bn_resblk32" 
  top: "bn_resblk32" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32 
  name: "relu_bn_resblk32" 
  type: "ReLU" 
  bottom: "bn_resblk32" 
  top: "bn_resblk32" 
} 
layer { # resblk32_b 
  name: "resblk32_b" 
  type: "Convolution" 
  bottom: "bn_resblk32" 
  top: "resblk32_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 16 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_b 
  name: "batchNorm_resblk32_b" 
  type: "BatchNorm" 
  bottom: "resblk32_b" 
  top: "bn_resblk32_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_b 
  name: "batchNorm_resblk32_b" 
  type: "BatchNorm" 
  bottom: "resblk32_b" 
  top: "bn_resblk32_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_b 
  name: "scale_resblk32_b" 
  type: "Scale" 
  bottom: "bn_resblk32_b" 
  top: "bn_resblk32_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # pool_resblk32 
  name: "avePooling_resblk32" 
  type: "Pooling" 
  bottom: "sum_bn_Conv16_9_b" 
  top: "avgPool_resblk32" 
  pooling_param { 
    pool: AVE 
    kernel_size: 3 
    stride: 2 
  } 
} 
layer { # sum_avgPool_resblk32 
    name: "sum_avgPool_resblk32" 
    type: "Eltwise" 
    bottom: "avgPool_resblk32" 
    bottom: "bn_resblk32_b" 
    top: "sum_bn_resblk32_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_b 
  name: "relu_sum_bn_resblk32_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_b" 
  top: "sum_bn_resblk32_b" 
} 
layer { # Dummy 
  name: "zeros_sum_bn_resblk32_b" 
  type: "DummyData" 
  top: "zeros_sum_bn_resblk32_b" 
  dummy_data_param { 
    shape: {dim: 125  dim: 16 dim: 16  dim: 16 } 
    data_filler: { 
                type: "constant" 
                value: 0 
        } 
  } 
} 
layer { # ConCat_sum_bn_resblk32_b 
  name: "CC_sum_bn_resblk32_b" 
  bottom: "sum_bn_resblk32_b" 
  bottom: "zeros_sum_bn_resblk32_b" 
  top: "CC_sum_bn_resblk32_b" 
  type: "Concat" 
  concat_param { 
    axis: 1 
  } 
} 
layer { # resblk32_1 
  name: "resblk32_1" 
  type: "Convolution" 
  bottom: "CC_sum_bn_resblk32_b" 
  top: "resblk32_1" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_1 
  name: "batchNorm_resblk32_1" 
  type: "BatchNorm" 
  bottom: "resblk32_1" 
  top: "bn_resblk32_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_1 
  name: "batchNorm_resblk32_1" 
  type: "BatchNorm" 
  bottom: "resblk32_1" 
  top: "bn_resblk32_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_1 
  name: "scale_resblk32_1" 
  type: "Scale" 
  bottom: "bn_resblk32_1" 
  top: "bn_resblk32_1" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_1 
  name: "relu_bn_resblk32_1" 
  type: "ReLU" 
  bottom: "bn_resblk32_1" 
  top: "bn_resblk32_1" 
} 
layer { # resblk32_1_b 
  name: "resblk32_1_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_1" 
  top: "resblk32_1_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_1_b 
  name: "batchNorm_resblk32_1_b" 
  type: "BatchNorm" 
  bottom: "resblk32_1_b" 
  top: "bn_resblk32_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_1_b 
  name: "batchNorm_resblk32_1_b" 
  type: "BatchNorm" 
  bottom: "resblk32_1_b" 
  top: "bn_resblk32_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_1_b 
  name: "scale_resblk32_1_b" 
  type: "Scale" 
  bottom: "bn_resblk32_1_b" 
  top: "bn_resblk32_1_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_CC_sum_bn_resblk32_b 
    name: "sum_CC_sum_bn_resblk32_b" 
    type: "Eltwise" 
    bottom: "CC_sum_bn_resblk32_b" 
    bottom: "bn_resblk32_1_b" 
    top: "sum_bn_resblk32_1_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_1_b 
  name: "relu_sum_bn_resblk32_1_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_1_b" 
  top: "sum_bn_resblk32_1_b" 
} 
layer { # resblk32_2 
  name: "resblk32_2" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_1_b" 
  top: "resblk32_2" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_2 
  name: "batchNorm_resblk32_2" 
  type: "BatchNorm" 
  bottom: "resblk32_2" 
  top: "bn_resblk32_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_2 
  name: "batchNorm_resblk32_2" 
  type: "BatchNorm" 
  bottom: "resblk32_2" 
  top: "bn_resblk32_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_2 
  name: "scale_resblk32_2" 
  type: "Scale" 
  bottom: "bn_resblk32_2" 
  top: "bn_resblk32_2" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_2 
  name: "relu_bn_resblk32_2" 
  type: "ReLU" 
  bottom: "bn_resblk32_2" 
  top: "bn_resblk32_2" 
} 
layer { # resblk32_2_b 
  name: "resblk32_2_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_2" 
  top: "resblk32_2_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_2_b 
  name: "batchNorm_resblk32_2_b" 
  type: "BatchNorm" 
  bottom: "resblk32_2_b" 
  top: "bn_resblk32_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_2_b 
  name: "batchNorm_resblk32_2_b" 
  type: "BatchNorm" 
  bottom: "resblk32_2_b" 
  top: "bn_resblk32_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true    
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_2_b 
  name: "scale_resblk32_2_b" 
  type: "Scale" 
  bottom: "bn_resblk32_2_b" 
  top: "bn_resblk32_2_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_1_b 
    name: "sum_sum_bn_resblk32_1_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_1_b" 
    bottom: "bn_resblk32_2_b" 
    top: "sum_bn_resblk32_2_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_2_b 
  name: "relu_sum_bn_resblk32_2_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_2_b" 
  top: "sum_bn_resblk32_2_b" 
} 
layer { # resblk32_3 
  name: "resblk32_3" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_2_b" 
  top: "resblk32_3" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_3 
  name: "batchNorm_resblk32_3" 
  type: "BatchNorm" 
  bottom: "resblk32_3" 
  top: "bn_resblk32_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_3 
  name: "batchNorm_resblk32_3" 
  type: "BatchNorm" 
  bottom: "resblk32_3" 
  top: "bn_resblk32_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true   
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_3 
  name: "scale_resblk32_3" 
  type: "Scale" 
  bottom: "bn_resblk32_3" 
  top: "bn_resblk32_3" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_3 
  name: "relu_bn_resblk32_3" 
  type: "ReLU" 
  bottom: "bn_resblk32_3" 
  top: "bn_resblk32_3" 
} 
layer { # resblk32_3_b 
  name: "resblk32_3_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_3" 
  top: "resblk32_3_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_3_b 
  name: "batchNorm_resblk32_3_b" 
  type: "BatchNorm" 
  bottom: "resblk32_3_b" 
  top: "bn_resblk32_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_3_b 
  name: "batchNorm_resblk32_3_b" 
  type: "BatchNorm" 
  bottom: "resblk32_3_b" 
  top: "bn_resblk32_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_3_b 
  name: "scale_resblk32_3_b" 
  type: "Scale" 
  bottom: "bn_resblk32_3_b" 
  top: "bn_resblk32_3_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_2_b 
    name: "sum_sum_bn_resblk32_2_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_2_b" 
    bottom: "bn_resblk32_3_b" 
    top: "sum_bn_resblk32_3_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_3_b 
  name: "relu_sum_bn_resblk32_3_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_3_b" 
  top: "sum_bn_resblk32_3_b" 
} 
layer { # resblk32_4 
  name: "resblk32_4" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_3_b" 
  top: "resblk32_4" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_4 
  name: "batchNorm_resblk32_4" 
  type: "BatchNorm" 
  bottom: "resblk32_4" 
  top: "bn_resblk32_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_4 
  name: "batchNorm_resblk32_4" 
  type: "BatchNorm" 
  bottom: "resblk32_4" 
  top: "bn_resblk32_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_4 
  name: "scale_resblk32_4" 
  type: "Scale" 
  bottom: "bn_resblk32_4" 
  top: "bn_resblk32_4" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_4 
  name: "relu_bn_resblk32_4" 
  type: "ReLU" 
  bottom: "bn_resblk32_4" 
  top: "bn_resblk32_4" 
} 
layer { # resblk32_4_b 
  name: "resblk32_4_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_4" 
  top: "resblk32_4_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_4_b 
  name: "batchNorm_resblk32_4_b" 
  type: "BatchNorm" 
  bottom: "resblk32_4_b" 
  top: "bn_resblk32_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_4_b 
  name: "batchNorm_resblk32_4_b" 
  type: "BatchNorm" 
  bottom: "resblk32_4_b" 
  top: "bn_resblk32_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_4_b 
  name: "scale_resblk32_4_b" 
  type: "Scale" 
  bottom: "bn_resblk32_4_b" 
  top: "bn_resblk32_4_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_3_b 
    name: "sum_sum_bn_resblk32_3_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_3_b" 
    bottom: "bn_resblk32_4_b" 
    top: "sum_bn_resblk32_4_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_4_b 
  name: "relu_sum_bn_resblk32_4_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_4_b" 
  top: "sum_bn_resblk32_4_b" 
} 
layer { # resblk32_5 
  name: "resblk32_5" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_4_b" 
  top: "resblk32_5" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_5 
  name: "batchNorm_resblk32_5" 
  type: "BatchNorm" 
  bottom: "resblk32_5" 
  top: "bn_resblk32_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_5 
  name: "batchNorm_resblk32_5" 
  type: "BatchNorm" 
  bottom: "resblk32_5" 
  top: "bn_resblk32_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_5 
  name: "scale_resblk32_5" 
  type: "Scale" 
  bottom: "bn_resblk32_5" 
  top: "bn_resblk32_5" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_5 
  name: "relu_bn_resblk32_5" 
  type: "ReLU" 
  bottom: "bn_resblk32_5" 
  top: "bn_resblk32_5" 
} 
layer { # resblk32_5_b 
  name: "resblk32_5_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_5" 
  top: "resblk32_5_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_5_b 
  name: "batchNorm_resblk32_5_b" 
  type: "BatchNorm" 
  bottom: "resblk32_5_b" 
  top: "bn_resblk32_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_5_b 
  name: "batchNorm_resblk32_5_b" 
  type: "BatchNorm" 
  bottom: "resblk32_5_b" 
  top: "bn_resblk32_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_5_b 
  name: "scale_resblk32_5_b" 
  type: "Scale" 
  bottom: "bn_resblk32_5_b" 
  top: "bn_resblk32_5_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_4_b 
    name: "sum_sum_bn_resblk32_4_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_4_b" 
    bottom: "bn_resblk32_5_b" 
    top: "sum_bn_resblk32_5_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_5_b 
  name: "relu_sum_bn_resblk32_5_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_5_b" 
  top: "sum_bn_resblk32_5_b" 
} 
layer { # resblk32_6 
  name: "resblk32_6" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_5_b" 
  top: "resblk32_6" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_6 
  name: "batchNorm_resblk32_6" 
  type: "BatchNorm" 
  bottom: "resblk32_6" 
  top: "bn_resblk32_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_6 
  name: "batchNorm_resblk32_6" 
  type: "BatchNorm" 
  bottom: "resblk32_6" 
  top: "bn_resblk32_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_6 
  name: "scale_resblk32_6" 
  type: "Scale" 
  bottom: "bn_resblk32_6" 
  top: "bn_resblk32_6" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_6 
  name: "relu_bn_resblk32_6" 
  type: "ReLU" 
  bottom: "bn_resblk32_6" 
  top: "bn_resblk32_6" 
} 
layer { # resblk32_6_b 
  name: "resblk32_6_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_6" 
  top: "resblk32_6_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_6_b 
  name: "batchNorm_resblk32_6_b" 
  type: "BatchNorm" 
  bottom: "resblk32_6_b" 
  top: "bn_resblk32_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_6_b 
  name: "batchNorm_resblk32_6_b" 
  type: "BatchNorm" 
  bottom: "resblk32_6_b" 
  top: "bn_resblk32_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_6_b 
  name: "scale_resblk32_6_b" 
  type: "Scale" 
  bottom: "bn_resblk32_6_b" 
  top: "bn_resblk32_6_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_5_b 
    name: "sum_sum_bn_resblk32_5_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_5_b" 
    bottom: "bn_resblk32_6_b" 
    top: "sum_bn_resblk32_6_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_6_b 
  name: "relu_sum_bn_resblk32_6_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_6_b" 
  top: "sum_bn_resblk32_6_b" 
} 
layer { # resblk32_7 
  name: "resblk32_7" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_6_b" 
  top: "resblk32_7" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_7 
  name: "batchNorm_resblk32_7" 
  type: "BatchNorm" 
  bottom: "resblk32_7" 
  top: "bn_resblk32_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_7 
  name: "batchNorm_resblk32_7" 
  type: "BatchNorm" 
  bottom: "resblk32_7" 
  top: "bn_resblk32_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_7 
  name: "scale_resblk32_7" 
  type: "Scale" 
  bottom: "bn_resblk32_7" 
  top: "bn_resblk32_7" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_7 
  name: "relu_bn_resblk32_7" 
  type: "ReLU" 
  bottom: "bn_resblk32_7" 
  top: "bn_resblk32_7" 
} 
layer { # resblk32_7_b 
  name: "resblk32_7_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_7" 
  top: "resblk32_7_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_7_b 
  name: "batchNorm_resblk32_7_b" 
  type: "BatchNorm" 
  bottom: "resblk32_7_b" 
  top: "bn_resblk32_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_7_b 
  name: "batchNorm_resblk32_7_b" 
  type: "BatchNorm" 
  bottom: "resblk32_7_b" 
  top: "bn_resblk32_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_7_b 
  name: "scale_resblk32_7_b" 
  type: "Scale" 
  bottom: "bn_resblk32_7_b" 
  top: "bn_resblk32_7_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_6_b 
    name: "sum_sum_bn_resblk32_6_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_6_b" 
    bottom: "bn_resblk32_7_b" 
    top: "sum_bn_resblk32_7_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_7_b 
  name: "relu_sum_bn_resblk32_7_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_7_b" 
  top: "sum_bn_resblk32_7_b" 
} 
layer { # resblk32_8 
  name: "resblk32_8" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_7_b" 
  top: "resblk32_8" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_8 
  name: "batchNorm_resblk32_8" 
  type: "BatchNorm" 
  bottom: "resblk32_8" 
  top: "bn_resblk32_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_8 
  name: "batchNorm_resblk32_8" 
  type: "BatchNorm" 
  bottom: "resblk32_8" 
  top: "bn_resblk32_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_8 
  name: "scale_resblk32_8" 
  type: "Scale" 
  bottom: "bn_resblk32_8" 
  top: "bn_resblk32_8" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk32_8 
  name: "relu_bn_resblk32_8" 
  type: "ReLU" 
  bottom: "bn_resblk32_8" 
  top: "bn_resblk32_8" 
} 
layer { # resblk32_8_b 
  name: "resblk32_8_b" 
  type: "Convolution" 
  bottom: "bn_resblk32_8" 
  top: "resblk32_8_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk32_8_b 
  name: "batchNorm_resblk32_8_b" 
  type: "BatchNorm" 
  bottom: "resblk32_8_b" 
  top: "bn_resblk32_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk32_8_b 
  name: "batchNorm_resblk32_8_b" 
  type: "BatchNorm" 
  bottom: "resblk32_8_b" 
  top: "bn_resblk32_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk32_8_b 
  name: "scale_resblk32_8_b" 
  type: "Scale" 
  bottom: "bn_resblk32_8_b" 
  top: "bn_resblk32_8_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk32_7_b 
    name: "sum_sum_bn_resblk32_7_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk32_7_b" 
    bottom: "bn_resblk32_8_b" 
    top: "sum_bn_resblk32_8_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk32_8_b 
  name: "relu_sum_bn_resblk32_8_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk32_8_b" 
  top: "sum_bn_resblk32_8_b" 
} 
layer { # resblk64 
  name: "resblk64" 
  type: "Convolution" 
  bottom: "sum_bn_resblk32_8_b" 
  top: "resblk64" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 2 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64 
  name: "batchNorm_resblk64" 
  type: "BatchNorm" 
  bottom: "resblk64" 
  top: "bn_resblk64" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64 
  name: "batchNorm_resblk64" 
  type: "BatchNorm" 
  bottom: "resblk64" 
  top: "bn_resblk64" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64 
  name: "scale_resblk64" 
  type: "Scale" 
  bottom: "bn_resblk64" 
  top: "bn_resblk64" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64 
  name: "relu_bn_resblk64" 
  type: "ReLU" 
  bottom: "bn_resblk64" 
  top: "bn_resblk64" 
} 
layer { # resblk64_b 
  name: "resblk64_b" 
  type: "Convolution" 
  bottom: "bn_resblk64" 
  top: "resblk64_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 32 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_b 
  name: "batchNorm_resblk64_b" 
  type: "BatchNorm" 
  bottom: "resblk64_b" 
  top: "bn_resblk64_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_b 
  name: "batchNorm_resblk64_b" 
  type: "BatchNorm" 
  bottom: "resblk64_b" 
  top: "bn_resblk64_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_b 
  name: "scale_resblk64_b" 
  type: "Scale" 
  bottom: "bn_resblk64_b" 
  top: "bn_resblk64_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # pool_resblk64 
  name: "avePooling_resblk64" 
  type: "Pooling" 
  bottom: "sum_bn_resblk32_8_b" 
  top: "avgPool_resblk64" 
  pooling_param { 
    pool: AVE 
    kernel_size: 3 
    stride: 2 
  } 
} 
layer { # sum_avgPool_resblk64 
    name: "sum_avgPool_resblk64" 
    type: "Eltwise" 
    bottom: "avgPool_resblk64" 
    bottom: "bn_resblk64_b" 
    top: "sum_bn_resblk64_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_b 
  name: "relu_sum_bn_resblk64_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_b" 
  top: "sum_bn_resblk64_b" 
} 
layer { # Dummy 
  name: "zeros_sum_bn_resblk64_b" 
  type: "DummyData" 
  top: "zeros_sum_bn_resblk64_b" 
  dummy_data_param { 
    shape: {dim: 125  dim: 32 dim: 8  dim: 8 } 
    data_filler: { 
                type: "constant" 
                value: 0 
        } 
  } 
} 
layer { # ConCat_sum_bn_resblk64_b 
  name: "CC_sum_bn_resblk64_b" 
  bottom: "sum_bn_resblk64_b" 
  bottom: "zeros_sum_bn_resblk64_b" 
  top: "CC_sum_bn_resblk64_b" 
  type: "Concat" 
  concat_param { 
    axis: 1 
  } 
} 
layer { # resblk64_1 
  name: "resblk64_1" 
  type: "Convolution" 
  bottom: "CC_sum_bn_resblk64_b" 
  top: "resblk64_1" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_1 
  name: "batchNorm_resblk64_1" 
  type: "BatchNorm" 
  bottom: "resblk64_1" 
  top: "bn_resblk64_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_1 
  name: "batchNorm_resblk64_1" 
  type: "BatchNorm" 
  bottom: "resblk64_1" 
  top: "bn_resblk64_1" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_1 
  name: "scale_resblk64_1" 
  type: "Scale" 
  bottom: "bn_resblk64_1" 
  top: "bn_resblk64_1" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_1 
  name: "relu_bn_resblk64_1" 
  type: "ReLU" 
  bottom: "bn_resblk64_1" 
  top: "bn_resblk64_1" 
} 
layer { # resblk64_1_b 
  name: "resblk64_1_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_1" 
  top: "resblk64_1_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_1_b 
  name: "batchNorm_resblk64_1_b" 
  type: "BatchNorm" 
  bottom: "resblk64_1_b" 
  top: "bn_resblk64_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_1_b 
  name: "batchNorm_resblk64_1_b" 
  type: "BatchNorm" 
  bottom: "resblk64_1_b" 
  top: "bn_resblk64_1_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_1_b 
  name: "scale_resblk64_1_b" 
  type: "Scale" 
  bottom: "bn_resblk64_1_b" 
  top: "bn_resblk64_1_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_CC_sum_bn_resblk64_b 
    name: "sum_CC_sum_bn_resblk64_b" 
    type: "Eltwise" 
    bottom: "CC_sum_bn_resblk64_b" 
    bottom: "bn_resblk64_1_b" 
    top: "sum_bn_resblk64_1_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_1_b 
  name: "relu_sum_bn_resblk64_1_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_1_b" 
  top: "sum_bn_resblk64_1_b" 
} 
layer { # resblk64_2 
  name: "resblk64_2" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_1_b" 
  top: "resblk64_2" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_2 
  name: "batchNorm_resblk64_2" 
  type: "BatchNorm" 
  bottom: "resblk64_2" 
  top: "bn_resblk64_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_2 
  name: "batchNorm_resblk64_2" 
  type: "BatchNorm" 
  bottom: "resblk64_2" 
  top: "bn_resblk64_2" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_2 
  name: "scale_resblk64_2" 
  type: "Scale" 
  bottom: "bn_resblk64_2" 
  top: "bn_resblk64_2" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_2 
  name: "relu_bn_resblk64_2" 
  type: "ReLU" 
  bottom: "bn_resblk64_2" 
  top: "bn_resblk64_2" 
} 
layer { # resblk64_2_b 
  name: "resblk64_2_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_2" 
  top: "resblk64_2_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_2_b 
  name: "batchNorm_resblk64_2_b" 
  type: "BatchNorm" 
  bottom: "resblk64_2_b" 
  top: "bn_resblk64_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_2_b 
  name: "batchNorm_resblk64_2_b" 
  type: "BatchNorm" 
  bottom: "resblk64_2_b" 
  top: "bn_resblk64_2_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_2_b 
  name: "scale_resblk64_2_b" 
  type: "Scale" 
  bottom: "bn_resblk64_2_b" 
  top: "bn_resblk64_2_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_1_b 
    name: "sum_sum_bn_resblk64_1_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_1_b" 
    bottom: "bn_resblk64_2_b" 
    top: "sum_bn_resblk64_2_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_2_b 
  name: "relu_sum_bn_resblk64_2_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_2_b" 
  top: "sum_bn_resblk64_2_b" 
} 
layer { # resblk64_3 
  name: "resblk64_3" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_2_b" 
  top: "resblk64_3" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_3 
  name: "batchNorm_resblk64_3" 
  type: "BatchNorm" 
  bottom: "resblk64_3" 
  top: "bn_resblk64_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_3 
  name: "batchNorm_resblk64_3" 
  type: "BatchNorm" 
  bottom: "resblk64_3" 
  top: "bn_resblk64_3" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_3 
  name: "scale_resblk64_3" 
  type: "Scale" 
  bottom: "bn_resblk64_3" 
  top: "bn_resblk64_3" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_3 
  name: "relu_bn_resblk64_3" 
  type: "ReLU" 
  bottom: "bn_resblk64_3" 
  top: "bn_resblk64_3" 
} 
layer { # resblk64_3_b 
  name: "resblk64_3_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_3" 
  top: "resblk64_3_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_3_b 
  name: "batchNorm_resblk64_3_b" 
  type: "BatchNorm" 
  bottom: "resblk64_3_b" 
  top: "bn_resblk64_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_3_b 
  name: "batchNorm_resblk64_3_b" 
  type: "BatchNorm" 
  bottom: "resblk64_3_b" 
  top: "bn_resblk64_3_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_3_b 
  name: "scale_resblk64_3_b" 
  type: "Scale" 
  bottom: "bn_resblk64_3_b" 
  top: "bn_resblk64_3_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_2_b 
    name: "sum_sum_bn_resblk64_2_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_2_b" 
    bottom: "bn_resblk64_3_b" 
    top: "sum_bn_resblk64_3_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_3_b 
  name: "relu_sum_bn_resblk64_3_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_3_b" 
  top: "sum_bn_resblk64_3_b" 
} 
layer { # resblk64_4 
  name: "resblk64_4" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_3_b" 
  top: "resblk64_4" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_4 
  name: "batchNorm_resblk64_4" 
  type: "BatchNorm" 
  bottom: "resblk64_4" 
  top: "bn_resblk64_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_4 
  name: "batchNorm_resblk64_4" 
  type: "BatchNorm" 
  bottom: "resblk64_4" 
  top: "bn_resblk64_4" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_4 
  name: "scale_resblk64_4" 
  type: "Scale" 
  bottom: "bn_resblk64_4" 
  top: "bn_resblk64_4" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_4 
  name: "relu_bn_resblk64_4" 
  type: "ReLU" 
  bottom: "bn_resblk64_4" 
  top: "bn_resblk64_4" 
} 
layer { # resblk64_4_b 
  name: "resblk64_4_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_4" 
  top: "resblk64_4_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_4_b 
  name: "batchNorm_resblk64_4_b" 
  type: "BatchNorm" 
  bottom: "resblk64_4_b" 
  top: "bn_resblk64_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_4_b 
  name: "batchNorm_resblk64_4_b" 
  type: "BatchNorm" 
  bottom: "resblk64_4_b" 
  top: "bn_resblk64_4_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_4_b 
  name: "scale_resblk64_4_b" 
  type: "Scale" 
  bottom: "bn_resblk64_4_b" 
  top: "bn_resblk64_4_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_3_b 
    name: "sum_sum_bn_resblk64_3_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_3_b" 
    bottom: "bn_resblk64_4_b" 
    top: "sum_bn_resblk64_4_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_4_b 
  name: "relu_sum_bn_resblk64_4_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_4_b" 
  top: "sum_bn_resblk64_4_b" 
} 
layer { # resblk64_5 
  name: "resblk64_5" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_4_b" 
  top: "resblk64_5" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_5 
  name: "batchNorm_resblk64_5" 
  type: "BatchNorm" 
  bottom: "resblk64_5" 
  top: "bn_resblk64_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_5 
  name: "batchNorm_resblk64_5" 
  type: "BatchNorm" 
  bottom: "resblk64_5" 
  top: "bn_resblk64_5" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_5 
  name: "scale_resblk64_5" 
  type: "Scale" 
  bottom: "bn_resblk64_5" 
  top: "bn_resblk64_5" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_5 
  name: "relu_bn_resblk64_5" 
  type: "ReLU" 
  bottom: "bn_resblk64_5" 
  top: "bn_resblk64_5" 
} 
layer { # resblk64_5_b 
  name: "resblk64_5_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_5" 
  top: "resblk64_5_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_5_b 
  name: "batchNorm_resblk64_5_b" 
  type: "BatchNorm" 
  bottom: "resblk64_5_b" 
  top: "bn_resblk64_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_5_b 
  name: "batchNorm_resblk64_5_b" 
  type: "BatchNorm" 
  bottom: "resblk64_5_b" 
  top: "bn_resblk64_5_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_5_b 
  name: "scale_resblk64_5_b" 
  type: "Scale" 
  bottom: "bn_resblk64_5_b" 
  top: "bn_resblk64_5_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_4_b 
    name: "sum_sum_bn_resblk64_4_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_4_b" 
    bottom: "bn_resblk64_5_b" 
    top: "sum_bn_resblk64_5_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_5_b 
  name: "relu_sum_bn_resblk64_5_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_5_b" 
  top: "sum_bn_resblk64_5_b" 
} 
layer { # resblk64_6 
  name: "resblk64_6" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_5_b" 
  top: "resblk64_6" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_6 
  name: "batchNorm_resblk64_6" 
  type: "BatchNorm" 
  bottom: "resblk64_6" 
  top: "bn_resblk64_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_6 
  name: "batchNorm_resblk64_6" 
  type: "BatchNorm" 
  bottom: "resblk64_6" 
  top: "bn_resblk64_6" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_6 
  name: "scale_resblk64_6" 
  type: "Scale" 
  bottom: "bn_resblk64_6" 
  top: "bn_resblk64_6" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_6 
  name: "relu_bn_resblk64_6" 
  type: "ReLU" 
  bottom: "bn_resblk64_6" 
  top: "bn_resblk64_6" 
} 
layer { # resblk64_6_b 
  name: "resblk64_6_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_6" 
  top: "resblk64_6_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_6_b 
  name: "batchNorm_resblk64_6_b" 
  type: "BatchNorm" 
  bottom: "resblk64_6_b" 
  top: "bn_resblk64_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_6_b 
  name: "batchNorm_resblk64_6_b" 
  type: "BatchNorm" 
  bottom: "resblk64_6_b" 
  top: "bn_resblk64_6_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_6_b 
  name: "scale_resblk64_6_b" 
  type: "Scale" 
  bottom: "bn_resblk64_6_b" 
  top: "bn_resblk64_6_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_5_b 
    name: "sum_sum_bn_resblk64_5_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_5_b" 
    bottom: "bn_resblk64_6_b" 
    top: "sum_bn_resblk64_6_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_6_b 
  name: "relu_sum_bn_resblk64_6_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_6_b" 
  top: "sum_bn_resblk64_6_b" 
} 
layer { # resblk64_7 
  name: "resblk64_7" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_6_b" 
  top: "resblk64_7" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_7 
  name: "batchNorm_resblk64_7" 
  type: "BatchNorm" 
  bottom: "resblk64_7" 
  top: "bn_resblk64_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_7 
  name: "batchNorm_resblk64_7" 
  type: "BatchNorm" 
  bottom: "resblk64_7" 
  top: "bn_resblk64_7" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_7 
  name: "scale_resblk64_7" 
  type: "Scale" 
  bottom: "bn_resblk64_7" 
  top: "bn_resblk64_7" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_7 
  name: "relu_bn_resblk64_7" 
  type: "ReLU" 
  bottom: "bn_resblk64_7" 
  top: "bn_resblk64_7" 
} 
layer { # resblk64_7_b 
  name: "resblk64_7_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_7" 
  top: "resblk64_7_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_7_b 
  name: "batchNorm_resblk64_7_b" 
  type: "BatchNorm" 
  bottom: "resblk64_7_b" 
  top: "bn_resblk64_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_7_b 
  name: "batchNorm_resblk64_7_b" 
  type: "BatchNorm" 
  bottom: "resblk64_7_b" 
  top: "bn_resblk64_7_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_7_b 
  name: "scale_resblk64_7_b" 
  type: "Scale" 
  bottom: "bn_resblk64_7_b" 
  top: "bn_resblk64_7_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_6_b 
    name: "sum_sum_bn_resblk64_6_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_6_b" 
    bottom: "bn_resblk64_7_b" 
    top: "sum_bn_resblk64_7_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_7_b 
  name: "relu_sum_bn_resblk64_7_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_7_b" 
  top: "sum_bn_resblk64_7_b" 
} 
layer { # resblk64_8 
  name: "resblk64_8" 
  type: "Convolution" 
  bottom: "sum_bn_resblk64_7_b" 
  top: "resblk64_8" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_8 
  name: "batchNorm_resblk64_8" 
  type: "BatchNorm" 
  bottom: "resblk64_8" 
  top: "bn_resblk64_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_8 
  name: "batchNorm_resblk64_8" 
  type: "BatchNorm" 
  bottom: "resblk64_8" 
  top: "bn_resblk64_8" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_8 
  name: "scale_resblk64_8" 
  type: "Scale" 
  bottom: "bn_resblk64_8" 
  top: "bn_resblk64_8" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # relu_bn_resblk64_8 
  name: "relu_bn_resblk64_8" 
  type: "ReLU" 
  bottom: "bn_resblk64_8" 
  top: "bn_resblk64_8" 
} 
layer { # resblk64_8_b 
  name: "resblk64_8_b" 
  type: "Convolution" 
  bottom: "bn_resblk64_8" 
  top: "resblk64_8_b" 
  param { 
    lr_mult: 1 
    decay_mult: 1 
  } 
  param { 
    lr_mult: 2 
    decay_mult: 1 
  } 
  convolution_param { 
    num_output: 64 
    pad: 1 
    kernel_size: 3 
    stride: 1 
    weight_filler { 
      type: "msra" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # bn_resblk64_8_b 
  name: "batchNorm_resblk64_8_b" 
  type: "BatchNorm" 
  bottom: "resblk64_8_b" 
  top: "bn_resblk64_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TRAIN
  }
  batch_norm_param { 
    use_global_stats: false      
    moving_average_fraction: 0.999
  } 
} 
layer { # bn_resblk64_8_b 
  name: "batchNorm_resblk64_8_b" 
  type: "BatchNorm" 
  bottom: "resblk64_8_b" 
  top: "bn_resblk64_8_b" 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
  param { 
    lr_mult: 0 
    decay_mult: 0 
  } 
    include {
    phase: TEST
  }
  batch_norm_param { 
    use_global_stats: true      
    moving_average_fraction: 0.999
  } 
} 
layer { # scale_resblk64_8_b 
  name: "scale_resblk64_8_b" 
  type: "Scale" 
  bottom: "bn_resblk64_8_b" 
  top: "bn_resblk64_8_b" 
  scale_param { 
    bias_term: true 
  } 
} 
layer { # sum_sum_bn_resblk64_7_b 
    name: "sum_sum_bn_resblk64_7_b" 
    type: "Eltwise" 
    bottom: "sum_bn_resblk64_7_b" 
    bottom: "bn_resblk64_8_b" 
    top: "sum_bn_resblk64_8_b" 
    eltwise_param { 
    operation: SUM 
    } 
} 
layer { # relu_sum_bn_resblk64_8_b 
  name: "relu_sum_bn_resblk64_8_b" 
  type: "ReLU" 
  bottom: "sum_bn_resblk64_8_b" 
  top: "sum_bn_resblk64_8_b" 
} 
layer { # pool_resblk64_8 
  name: "avePooling_resblk64_8" 
  type: "Pooling" 
  bottom: "sum_bn_resblk64_8_b" 
  top: "avgPool_resblk64_8" 
  pooling_param { 
    pool: AVE 
    kernel_size: 8 
    stride: 1 
  } 
} 
layer { # FC_final 
  name: "FC_final" 
  type: "InnerProduct" 
  bottom: "avgPool_resblk64_8" 
  top: "FC_final" 
  param { 
    lr_mult: 1 
  } 
  param { 
    lr_mult: 2 
  } 
  inner_product_param { 
    num_output: 10 
    weight_filler { 
      type: "xavier" 
    } 
    bias_filler { 
      type: "constant" 
    } 
  } 
} 
layer { # accuracy 
  name: "accuracy" 
  type: "Accuracy" 
  bottom: "FC_final" 
  bottom: "label" 
  top: "accuracy" 
} 
layer { # loss 
  name: "loss" 
  type: "SoftmaxWithLoss" 
  bottom: "FC_final" 
  bottom: "label" 
  top: "loss" 
} 
