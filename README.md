# cifarSOTA

Reviewing recent advances in classification on CIFAR 10 and 100 datasets

# Table of Contents
0. [Super Convergence](#super-convergence)
1. [Stochastic Weight Averaging](#stochastic-weight-averaging)
2. [Born again networks](#born-again-networks)
3. [Erase ReLU](#erase-relu)
4. [Cyclic learning rates](#cyclic-learning-rates)
5. [Shake Drop](#shake-drop)
6. [Shake Shake](#shake-shake)
7. [mixup](#mixup)
8. [Cutout](#cutout)
9. [Random erasing](#random-erasing)
10. [SGDR](#sgdr)
11. [Densenets](#densenets)
12. [Pyramid drop](#pyramid-drop)
13. [Pyramid Resnet](#pyramid-resnet)
14. [ResNext](#resnext)
15. [Wide Resnet](#wide-resnet)
16. [Preact Resnet](#preact-resnet)
17. [Stochastic depth Resnets](#stochastic-depth-resnets)
18. [Resnet](#resnet)

## Super Convergence

[paper](https://arxiv.org/pdf/1708.07120v2.pdf) | [caffe official](https://github.com/lnsmith54/super-convergence)

![](images/super.png)

## Stochastic Weight Averaging

[paper](https://arxiv.org/abs/1803.05407) | [pytorch](https://github.com/timgaripov/swa) | 

![](images/swa.png)


## Born again networks

[paper](https://arxiv.org/pdf/1805.04770.pdf) |  

![](images/ban_cifar10.png)

![](images/born_again.png)


## Erase ReLU

[paper](https://arxiv.org/pdf/1709.07634.pdf) |

![](images/erase_relu.png)

# Cyclic learning rates

[paper](https://arxiv.org/abs/1506.01186)

![](images/clr.png)


## Shake Drop

[paper](https://arxiv.org/abs/1802.02375) | [lua official](https://github.com/imenurok/ShakeDrop) | 


![](images/shakedrop.png)


## Shake Shake

[paper](https://arxiv.org/abs/1705.07485) | [lua official](https://github.com/xgastaldi/shake-shake) | [pytorch](https://github.com/hysts/pytorch_shake_shake) | 


![](images/shake-shake.png)

![](images/shake_shake_cifar100.png)


## mixup 

[paper](https://arxiv.org/abs/1710.09412) | [pytorch official](https://github.com/hongyi-zhang/mixup) | 

![](images/mixup_cifar.png)


## Cutout

[paper](https://arxiv.org/abs/1708.04552) | [pytorch official](https://github.com/uoguelph-mlrg/Cutout) |

![](images/cutout.png)

## Random erasing

[paper](https://arxiv.org/abs/1708.04896) | [pytorch official](https://github.com/zhunzhong07/Random-Erasing) |


![](images/random_erasing.png)

# SGDR

[paper](https://arxiv.org/abs/1608.03983) | [official](https://github.com/loshchil/SGDR)

![](images/sgdr.png)
![](images/sgdr_ensembles.png)

# Densenets

[paper](https://arxiv.org/abs/1608.06993) | [official](https://github.com/liuzhuang13/DenseNet)

![](images/densenet.png)


# Pyramid drop

[paper](https://arxiv.org/pdf/1707.07074.pdf)

![](images/pyramid_drop.png)

# Pyramid Resnet

[paper](https://arxiv.org/abs/1610.02915)

![](images/pyramid.png)


# ResNext

[paper](https://arxiv.org/abs/1611.05431)

![](images/resnext.png)

# Wide Resnet

[paper](https://arxiv.org/abs/1605.07146)

![](images/wrn.png)

# Preact Resnet

[paper](https://arxiv.org/abs/1603.05027) |

![](images/preact_resnet.png)


# Stochastic depth Resnets

[paper](https://arxiv.org/abs/1603.09382) | [official](https://github.com/loshchil/SGDR)

![](images/stochastic.png)

# Resnet

[paper](https://arxiv.org/abs/1512.03385)

![](images/resnet.png)

